{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to learn BERT for RNA sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/sosuzcpzngwiknq/pair_dataset_large.tsv?dl=1 -O pair_dataset_large.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import DistilBertConfig, DistilBertForMaskedLM\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import notebook\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import LongTensor\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./pair_dataset_large.tsv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, stratify=df.sequence, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sequence'] = train['sequence'].apply(lambda x:x[:len(x)//2])\n",
    "test['sequence'] = test['sequence'].apply(lambda x:x[-len(x)//2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 128\n",
    "BATCH_SIZE = 4\n",
    "BATCHES_PER_STEP = 1\n",
    "DEVICE = torch.device('cpu') # torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 100 # df.sequence.apply(len).max() # make it smaller for better perfomance (worse score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARMUP_STEPS = 0\n",
    "TOTAL_SCHEDULER_STEPS = train.shape[0] * EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "EARLY_STOPPING_TOLERANCE = 0.01\n",
    "\n",
    "FITTED_THRESHOLD = 1e-8\n",
    "\n",
    "LOSS_LOG_EACH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write vocabulary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set.union(*df.sequence.apply(lambda x: set(x))))\n",
    "vocab_file = '\\n'.join(vocab)\n",
    "with open(\"./vocabulary.txt\",\"w\") as f:\n",
    "    f.write(vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistilBertConfig(vocab_size_or_config_json_file=len(vocab), dropout=DROPOUT, max_position_embeddings=MAXLEN)\n",
    "model = DistilBertForMaskedLM(config)\n",
    "tokenizer = BertTokenizer(\"./vocabulary.txt\",do_basic_tokenize=True, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment for cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cuda(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rfam(Dataset):\n",
    "    def __init__(self, sequences, question_masks, answer_masks, maxlen=512, vocabulary_file=\"./vocabulary.txt\"):\n",
    "        self.df = pd.DataFrame({'sequences': sequences, 'question_masks': question_masks, 'answer_masks': answer_masks})\n",
    "        self.tokenizer = BertTokenizer(vocabulary_file, do_basic_tokenize=True, do_lower_case=False)\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        encoded_seq = LongTensor(self.tokenizer.encode(' '.join(r.sequences[:self.maxlen]),add_special_tokens=False) + [0 for i in range(self.maxlen-len(r.sequences[:self.maxlen]))])\n",
    "        attention_mask = LongTensor([1]*len(encoded_seq) + [0]*(self.maxlen-len(encoded_seq)))\n",
    "        q, a = list(r.question_masks).index('1'), list(r.answer_masks).index('1')\n",
    "        return encoded_seq, attention_mask, q, a\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "train_data = rfam(train.sequence, train['mask'], train.ans, maxlen=MAXLEN)\n",
    "val_data = rfam(test.sequence, test['mask'], test.ans, maxlen=MAXLEN)\n",
    "batch_generator = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "validation_sampler = DataLoader(val_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = WARMUP_STEPS,\n",
    "                                            num_training_steps = TOTAL_SCHEDULER_STEPS)\n",
    "\n",
    "bad_epochs_es, bad_epochs_od, min_epoch_val_loss = 0, 0, 9000\n",
    "\n",
    "epoch_train_losses, epoch_val_losses = [], []\n",
    "epoch_train_loss_errs, epoch_val_loss_errs = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    losses, batch_idx, val_batch_idx = [], 0, 0\n",
    "        \n",
    "    for seq, attn, q, a in notebook.tqdm(batch_generator, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        prediction = model(seq, attention_mask=attn)[0].transpose(1,2).to(device=DEVICE)\n",
    "        loss = criterion(prediction, seq).to(device=DEVICE)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        if batch_idx%BATCHES_PER_STEP==0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()        \n",
    "        if batch_idx%LOSS_LOG_EACH==0:\n",
    "            epoch_train_loss = np.mean(losses)\n",
    "            epoch_train_losses.append(epoch_train_loss)\n",
    "            losses = []\n",
    "            epoch_train_loss_errs.append(np.std(losses)/np.sqrt(len(losses)))\n",
    "            val_losses = []\n",
    "            for seq, attn, _, _ in validation_sampler:\n",
    "                val_prediction = model(seq, attention_mask=attn)[0].transpose(1,2).to(device=DEVICE)\n",
    "                val_loss = criterion(val_prediction, seq).to(device=DEVICE)\n",
    "                val_losses.append(val_loss.item())\n",
    "            epoch_val_loss = np.mean(val_losses)\n",
    "            epoch_val_losses.append(epoch_val_loss)\n",
    "            val_losses = []\n",
    "            epoch_val_loss_errs.append(np.std(val_losses)/np.sqrt(len(val_losses)))\n",
    "        batch_idx+=1\n",
    "    \n",
    "    plt.clf()\n",
    "    clear_output()\n",
    "    x_axis = [(i+1)*LOSS_LOG_EACH for i in range(len(epoch_train_losses))]\n",
    "    plt.errorbar(x=x_axis,y=epoch_train_losses, yerr=epoch_train_loss_errs, fmt='o-', capsize=10, label=\"Train\")\n",
    "    plt.errorbar(x=x_axis,y=epoch_val_losses, yerr=epoch_val_loss_errs, fmt='o-', capsize=10, label=\"Val\")\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    if epoch_val_loss >= min_epoch_val_loss + EARLY_STOPPING_TOLERANCE:\n",
    "        bad_epochs_es += 1\n",
    "    if epoch_val_loss < min_epoch_val_loss:\n",
    "        min_epoch_val_loss = epoch_val_loss\n",
    "        model.save_pretrained(\"bert_for_rna_seqs\")\n",
    "    if bad_epochs_es > EARLY_STOPPING_PATIENCE:\n",
    "        print(\"Break by early stopping\")\n",
    "        break\n",
    "    if epoch_val_loss < FITTED_THRESHOLD:\n",
    "        print(\"Break due to holdout loss being under threshold\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
