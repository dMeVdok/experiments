{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7577482f-6c38-40ee-b430-4c3c6c5a689c",
   "metadata": {},
   "source": [
    "Hi, this is just a quick submission without any rocket science involved. I will definitely make more submissions, but please consider this one for now. It should give around `.91` test accuracy, however, there is still no full determinism in both transformers library and pytorch. Note the [deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base) model I use here is opensource and is a base model.\n",
    "\n",
    "Dmitrii Evdokimov (dmevdok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be4150-64d8-4546-ac8a-12d738667e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a28b0b-75dd-4e73-9883-b58dafdab848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmevdok/anaconda3/envs/aitinkerers_comp/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2815' max='2815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2815/2815 19:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.347893</td>\n",
       "      <td>0.856354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>0.362159</td>\n",
       "      <td>0.882136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>0.395049</td>\n",
       "      <td>0.883978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1128</td>\n",
       "      <td>0.363200</td>\n",
       "      <td>0.482597</td>\n",
       "      <td>0.883978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.363200</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>0.885820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1692</td>\n",
       "      <td>0.260400</td>\n",
       "      <td>0.531736</td>\n",
       "      <td>0.893186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1974</td>\n",
       "      <td>0.260400</td>\n",
       "      <td>0.781823</td>\n",
       "      <td>0.893186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2256</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.848472</td>\n",
       "      <td>0.882136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2538</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.868692</td>\n",
       "      <td>0.895028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2815, training_loss=0.2785524737559669, metrics={'train_runtime': 1165.5339, 'train_samples_per_second': 38.609, 'train_steps_per_second': 2.415, 'total_flos': 0.0, 'train_loss': 0.2785524737559669, 'epoch': 5.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModel, AutoModelForSequenceClassification, DataCollatorWithPadding, DataCollator\n",
    "import os\n",
    "import evaluate\n",
    "from transformers import set_seed\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "class Embedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, trainable=True):\n",
    "        super().__init__()\n",
    "        self.embedding = AutoModel.from_pretrained('microsoft/deberta-v3-base', num_labels=3)\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        if not self.trainable:\n",
    "            with torch.no_grad():\n",
    "                return self.embedding(input_ids, attention_mask=attention_mask, output_hidden_states=True, output_attentions=True)\n",
    "        else:\n",
    "            return self.embedding(input_ids, attention_mask=attention_mask, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "class Wrapper(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, wrapped, embedding_size=768, n_classes=3, dropout=0.2, head_dimension=1000, random_state=42):\n",
    "        super().__init__()\n",
    "        self.random_state = random_state\n",
    "        self.wrapped = wrapped\n",
    "        self.n_transformer_layers = len(wrapped.embedding.encoder.layer)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_classes = n_classes\n",
    "        self.head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(\n",
    "                self.embedding_size*3,\n",
    "                self.embedding_size*3\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(\n",
    "                self.embedding_size*3,\n",
    "                self.n_classes\n",
    "            ),\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "        self.head[0].weight.data.normal_(mean=0., std=.02)\n",
    "        self.head[0].bias.data.zero_()\n",
    "        self.head[2].weight.data.normal_(mean=0., std=.02)\n",
    "        self.head[2].bias.data.zero_()\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input_ids, labels, attention_mask):\n",
    "        wrapped_out = self.wrapped(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out = wrapped_out.hidden_states\n",
    "        out = self.head(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    out[-1][:,0,:],\n",
    "                    out[-2][:,0,:],\n",
    "                    out[-3][:,0,:],\n",
    "                ],\n",
    "                -1\n",
    "            )\n",
    "        )\n",
    "        out = torch.cat(\n",
    "            [\n",
    "                out[:,0,None],\n",
    "                out[:,1,None],\n",
    "                out[:,2,None] \n",
    "            ],\n",
    "            -1\n",
    "        )\n",
    "        return {\n",
    "            'loss': self.criterion(out, labels),\n",
    "            'logits': out\n",
    "        }\n",
    "\n",
    "def model_init():\n",
    "    set_seed(SEED)\n",
    "    return  Wrapper(\n",
    "        Embedding()\n",
    "    )\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=3)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "dataset = load_dataset(\"zeroshot/twitter-financial-news-sentiment\").shuffle(SEED)\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer(t['text'])\n",
    "\n",
    "tokenized_data = dataset.map(tokenize, batched=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_steps=0.1,\n",
    "    evaluation_strategy=\"steps\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_data['train'].select(range(9000)),\n",
    "    eval_dataset = tokenized_data['train'].select(range(9000, 9543)),\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2badc9-31ce-4ac4-8c34-8599dbda01be",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db08e110-b0c1-4f24-bf8f-56ba5a937851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9158291457286433"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_prediction = trainer.predict(tokenized_data['validation'], ignore_keys=['label']).predictions\n",
    "\n",
    "accuracy_metric = lambda pred, true: sum(true == np.argmax(pred, 1)) / len(true)\n",
    "\n",
    "accuracy_metric(\n",
    "    submission_prediction,\n",
    "    dataset['validation']['label']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
